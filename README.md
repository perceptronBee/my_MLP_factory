# ğŸ§  my_MLP_factory

Basit bir **Python Multi-Layer Perceptron (MLP) sÄ±nÄ±fÄ±**.  
Kendi kÃ¼Ã§Ã¼k yapay sinir aÄŸÄ± deneylerini yapmak iÃ§in tasarlandÄ±. ğŸ¯

---

## âœ¨ Ã–zellikler

- Ã‡ok katmanlÄ± perceptron desteÄŸi (gizli ve Ã§Ä±kÄ±ÅŸ katmanlarÄ±)
- Sigmoid aktivasyon fonksiyonu
- Basit feedforward ve backpropagation
- Kendi veri setinle veya **XOR** gibi Ã¶rneklerle Ã§alÄ±ÅŸabilir

---

## âš¡ Kurulum

1. Python 3.x yÃ¼klÃ¼ olmalÄ± ğŸ  
2. DosyayÄ± indir veya klonla  
3. Ekstra paket yok, tamamen standart Python ile Ã§alÄ±ÅŸÄ±yor âœ”ï¸

---

## ğŸ› ï¸ KullanÄ±m Ã–rneÄŸi

```python
from my_MLP_factory import MLP

# XOR veri seti
data = [[0,0],[0,1],[1,0],[1,1]]
labels = [[0],[1],[1],[0]]

# MLP oluÅŸturuluyor
# [2,2,1] -> Katman yapÄ±sÄ±:
# 2 nÃ¶ronlu input layer (girdi sayÄ±sÄ±=2)
# 2 nÃ¶ronlu gizli layer
# 1 nÃ¶ronlu output layer
# 2 -> input katmanÄ±ndaki her bir nÃ¶ron iÃ§in giriÅŸ sayÄ±sÄ±
mlp = MLP([2,2,1], 2)

epochs = 50000  # kaÃ§ defa tÃ¼m veri seti Ã¼zerinden geÃ§ileceÄŸi
lr = 0.5        # Ã¶ÄŸrenme hÄ±zÄ±

for epoch in range(epochs):
    loss = mlp.train_epoch(data, labels, lr)
    if epoch % 10000 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.6f}")

print("\nXOR Test SonuÃ§larÄ±:")
for x, y in zip(data, labels):
    out = mlp.feedforward(x, y)
    print(f"{x} -> {out}")
